import gradio as gr
import requests

# å®šä¹‰åå°çš„fastapiçš„URL
backend_url = "http://127.0.0.1:6066/chat"

def chat_with_backend(prompt, history, sys_prompt, history_len, temperature, top_p, max_tokens, stream):
    # history:["role": "user", "metadata":{'title':None},"content":"xxxx"],å»æ‰metadataå­—æ®µ
    history_none_meatdata = [{"role": h.get("role"), "content": h.get("content")} for h in history]
    # print(history)
    # æ„å»ºè¯·æ±‚çš„æ•°æ®
    data = {
        "query": prompt,
        "sys_prompt": sys_prompt,
        "history": history_none_meatdata,
        "history_len": history_len, 
        "temperature": temperature,
        "top_p": top_p,
        "max_tokens": max_tokens
    }
    response = requests.post(backend_url, json=data, stream=True)
    if response.status_code == 200:
        chunks = ""
        if stream:
            for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                chunks += chunk
                yield chunks
        else:
            for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                chunks += chunk
            yield chunks


# ä½¿ç”¨gr.Blocksåˆ›å»ºä¸€ä¸ªå—ï¼Œå¹¶è®¾ç½®å¯ä»¥å¡«å……é«˜åº¦å’Œå®½åº¦
with gr.Blocks(fill_width=True, fill_height=True) as demo:
    # åˆ›å»ºä¸€ä¸ªæ ‡ç­¾é¡µ
    with gr.Tab("ğŸ¤– èŠå¤©æœºå™¨äºº"):
        # æ·»åŠ æ ‡é¢˜
        gr.Markdown("## ğŸ¤– èŠå¤©æœºå™¨äºº")

        # åˆ›å»ºä¸€ä¸ªè¡Œå¸ƒå±€
        with gr.Row():
            # åˆ›ä¸€ä¸ªå·¦ä¾§çš„åˆ—å¸ƒå±€
            with gr.Column(scale=1, variant="panel") as sidebar_left:
                sys_prompt = gr.Textbox(label="ç³»ç»Ÿæç¤ºè¯", value="You are a helpful assistant")
                history_len = gr.Slider(minimum=1, maximum=10, value=1, label="ä¿ç•™å†å²å¯¹è¯çš„æ•°é‡")
                temperature = gr.Slider(minimum=0.01, maximum=2.0, value=0.5, step=0.01, label="temperature")
                top_p = gr.Slider(minimum=0.01, maximum=1.0, value=0.5, step=0.01, label="top_p")
                max_tokens = gr.Slider(minimum=512, maximum=4096, value=1024, step=8, label="max_tokens")
                stream = gr.Checkbox(label="stream", value=True)
            
            # åˆ›å»ºå³ä¾§çš„åˆ—å¸ƒå±€ï¼Œè®¾ç½®æ¯”ä¾‹ä¸º10
            with gr.Column(scale=10) as main:
                # åˆ›å»ºèŠå¤©æœºå™¨äººçš„èŠå¤©ç•Œé¢ï¼Œé«˜åº¦ä¸º500px
                chatbot = gr.Chatbot(type="messages", height=500)
                # åˆ›å»ºchatinterface, ç”¨äºå¤„ç†èŠå¤©çš„é€»è¾‘
                gr.ChatInterface(fn=chat_with_backend,
                                 type="messages",
                                 chatbot=chatbot,
                                 additional_inputs=[
                                     sys_prompt,
                                     history_len,
                                     temperature,
                                     top_p,
                                     max_tokens,
                                     stream
                                 ])

demo.launch()

