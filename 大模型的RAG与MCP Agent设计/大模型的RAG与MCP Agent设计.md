#### 一、LangChain封装本地聊天模型

1、安装库

 	langchain：是一个用于构建对话系统的框架。它提供了许多工具和模块，帮助开发者快速搭建和训练对系统。这个库支持多种语言模型和服务，包括但不限于OpenAI的模型。

 	langchain\_community：是一个社区驱动的扩展库，旨在为langchain提供更多的功能和插件。这个库通常包含由社区贡献的模块和工具，可以帮助开发者更快地实现特定功能。

 	langchain-openai：是专门为langchain框架设计的OpenAI集成库。它提供了与OpenAI API交互的便捷方式，使得在langchain中使用OpenAI的模型变得更加简单。

2、LangChain模型推理

 	LangChain中包含两种语言模型：LLMs（纯文本补全模型）和ChatModels（专门针对对话进行了调整的模型）

3、LangChain部署本地模型方法有两种：

 	方法一：如果你启动vm服务，则vlm提供的api接口与OpenAl接口兼容可以直接使用。

 	方法二：使用手动封装的方式替掉openai(手动封装的效果等于llm.invoke的效果，也就是纯

文本补全)。

4、聊天模型为什么需要特殊符号：

 	这些符号主要是为了帮助模型区分不同的对话方。在多轮对话中，模型需要知道当前的发言人是用户还是系统，或者是自己（助手）。这种明确的角色标记让模型能够更好的理解对话的上下文并做出适当的响应。

#### 二、LangChain-prompt提示词

***1、在大模型应用中，什么是Prompt（提示）？它起什么作用？***

 	Prompt是一种包含文本指令和占位符的模版，用于指导大语言

模型（LLM）生成特定类型或格式的输出。它的主要作用是引导和约束模型的行为，使其产生符合预期的回复。

***2、LangChain中的 chatpromptTemplate 主要用于构建什么类型的提示?它通常由哪几种不同角色的消息模板构成?***

ChatPromptTemplate主要用于构建模拟多轮对话场景的结构化提示。它通常由系统消息模板（SystemMessagePromptTemplate）、人类（用户）消息模板（HumanMessagePromptTemplate）和AI（助手）消息模板（AIMessagePromptTemplate）\\\*\\\*组合而成。



***3、在 chatPromptTemplate.from messages(\[...])中，可以直接使用 SystemMessage.HumanMessage 等消息类也可以使用 systemMessagePromptTemplate 等模板类。这两者在使用上的主要区别是什么?***

 	主要区别在于是否包含变量占位符。

 	直接使用消息类（如SystemMessage），其内容是固定的。

 	使用消息模板类（如SystemMessagePromptTemplate）则可以包含占位符（如{variable}），允许在格式化时动态填充内容。

#### 三、LangChain-outputparser输出解释器

***1、在LangChain中，OutputParser(输出解析器)是用来做什么的?为什么它在大模型应用开发中很重要?***

 	OutputParser用于解析大语言模型（LLM）返回的文本结果，将其从非结构化的自然语言转换成程序可用的结构化数据（如列表、字典、对象）。

 	它很重要，因为应用程序通常需要结构化的数据来进行后续的逻辑处理，OutputParser使得从LLM获取可用数据更加方便和可靠。

#### 四、LangChain-chain链

***1、在LangChain中，“链”的主要概念是指？***

 	将多个处理步骤或者组件按顺序连接起来的方式，链的核心是将不同的功能模块串联起来，形成一个处理流程

***2、LangChain中最简单的内置链 LLMchain 是由哪两个基本组件构成的?它实现了什么样的基本流程?***

 	LLMChain 由一个 PromptTemplate（提示模板） 和一个 语言模型（LLM或聊天模型） 构成。

***3、什么是检索增强（RAG）技术？相比单纯的LLM生成，他有什么优势？***

 	RAG是一种将外部知识检索（Retrieval）与LLM的文本生成（Generation）相结合的技术。优势在于，它能让LLM在生成回答时利用检索到的最新、特定领域的外部信息作为上下文，从而生成更准确、更可靠、基于事实的答案，减少“幻觉”。

***4、LangChain中的RetrievalgQA 链是如何实现RAG功能的?它通常依赖哪些组件协同工作?***

 	LangChain中的RetrievalQA链通过标准化流水线实现RAG功能:

 	a、首先使用检索器从向量数据库中查找与问题相关的文档片段，

 	b、然后将这些检索结果作为上下文信息与用户问题一同填入预设的提示模板，

 	c、最后交由语言模型生成基于证据的准确回答。

 	这一流程依赖四个核心组件协同工作:文档加载器负责从多种来源获取原始数据，文本分割器将长文档切分为适合处理的片段，嵌入模型与向量数据库将文本转换为向量并实现相似性检索，提示模板与语言模型则共同完成信息整合与自然语言生成，形成端到端的知识增强问答系统。

#### 五、LangChain-memory记忆

1、什么是memory记忆？

 	memory在LangChain中是用来存储对话状态的组件，通常，在一个复杂的对话系统中，我们需要用户之前说过的话，以便生成上下文相关的回答。

 	LangChain提供了多种类型的memory机制，帮助开发者方便的管理对话历史或其他有状态的信息

2、三种类型：

 	a、conversationbuffermemory：

 	ConversationBufferMemory将所有轮次的对话历史完整地存储在一个缓冲区中。其主要特点是简单直接，保留了全部原始对话信息，但在对话很长时可能会超出模型的上下文长度限制。

 	b、conversationsummarymemory：

 	ConversationSummaryMemory不对对话历史进行完整存储，而是使用一个LLM实时地将对话内容进行总结摘要。它的优势在于能够有效压缩历史信息长度，减少Token消耗，更适合处理非常长的对话。

 	c、conversationbufferwindowsmemory：

 	ConversationBufferWindowMemory通过一个滑动窗口来管理对话历史，只保留最近的k轮对话交互记录。参数k决定了这个窗口的大小，即保留最近对话的数量。

#### 六、LangChain-agent代理

***1、在LangChain中，Agent(代理)和 Chain(链)的主要区别是什么?Agent具备哪些Chain通常不具备的能***

***力?***

Chain通常按照预定义的固定顺序执行组件。



 	Agent则使用LLM进行思考和决策，能够根据输入动态地选择并调用一个或多个工具（Tools），执行顺序不固定，直到任务完成。Agent具备自主规划和工具使用的能力。

2、定义一个LangChain Agent可以使用的Tool时，name，func 和 description 这三个参数分别有什么作用?

 	name是工具的唯一标识名；

 	func是工具实际执行的Python函数；

 	description是描述工具功能和适用场景的文本。

***3、很多LangChain Agent是基于ReAct框架实现的，请简述ReAct框架的工作流程(包含哪些关键步骤)？***

 	ReAct框架的工作流程是思考（Thought）-> 行动（Action）-> 行动输入（Action Input）-> 观察（Observation）的循环。Agent先思考该做什么，决定调用哪个工具及输入，执行后得到结果（观察），再根据结果进行下一步思考，直至产生最终答案（Final Answer）。

***4、在创建基于ReAct框架的Agent时，为什么通常需要设置 stop\_sequence 参数(例\["\\nobservation:"」)?***

因为ReAct流程需要Agent（LLM）在输出Action和Action Input后暂停，以便框架能解析这些信息并执行相应的Tool。设置stop\\\_sequence就是告诉LLM在生成到这个特定标记（如换行+Observation:）时停止输出，让框架接管执行Tool。



#### 七、RAG的介绍

1、什么是RAg？它的核心工作机制是什么？

 	rag是一种检索增强生成的技术，它是将信息检索技术和大模型推理生成技术相结合的先进框架。它的核心组件是检索器、生成器、知识库。

实现流程：首先对要添加的知识（文档）进行加载，随后进行分块处理，利用词嵌入模型对分块后的文档进行向量化，同时，将用户指令也进行向量化，一同输入到向量数据库中，进行向量检索，将检索后的上下文和原始查询给到LLM大模型进行预测输出。

 	RAg更像是给了模型一个知识外挂，模型在回答问题时不在是胡编乱造，而是从已经给出的外部知识库查找相关信息，然后根据查到的信息来组织答案

2、RAg模型通常包括三个主要组件：

 	检索器：检索器负责从大型外部知识库中找到当前任务或问题相关的文档，它通常使用诸如Elasticsearch或基于嵌入的向量搜索，（FAISS）来检索最相关的信息。知识库可以是任何形式的文本库，包括维基百科，公司内部文档，技术文档等

 	生成器：生成器是基于预训练的大规模语言模型，负责顾根据检索到的信息生成自然语言的回答或文本，生成器不仅依赖与模型本身的内部知识（即在训练过程中学到的知识）。还结合了从检索器获得的外部信息来生成更具有上下文喝准确性的答案。

&nbsp;	知识库：

#### 八、RAG的文本加载

***1、在LangChain RAG流程中，Document Loader(文档加载器)承担的首要任务是什么?***



 	首要任务是从不同的数据源（如TXT, PDF, CSV等文件）加载原始数据，并将这些数据转换成LangChain统一的Document对象格式，为后续的文本处理（如分割、嵌入）做准备。

***2、LangChain中用于表示加载后数据的 Document 对象，其结构通常包含哪两个主要部分?分别是什么含义?***

 	包含两个主要部分：page\_content 和 metadata。page\_content 存储文档的实际文本内容。metadata 是一个字典，存储文档的元数据信息。

***3、使用 PyPDFLoader 加载PDF文档时，调用 .load\_and\_split()方法有什么特点?这对于后续处理有什么好处?***

 	特点是它会将PDF按页分割，每一页生成一个Document对象，并在metadata中记录页码。好处是直接获得了以页为单位的文档块，方便后续按页进行信息检索或处理。

***4、在使用 unstructuredMarkdownLoader 加载Markdown文件时，设置 mode="elements"有什么作用?***

设置mode="elements"可以使加载器根据Markdown的语义结构（如标题、段落）来切分文档，生成多个Document对象。

#### 九、RAG的文本分割

***1、在RAG系统中，为什么需要对加载的原始文档进行文本分割?至少说出两点理由？***

 	1、提升检索效率：是文本分割的首要原因，对于较长的文档，将其分成多个片段或片段，这样每个片段可以单独进行检索，从而提高检索的精度和速度

 	2、更好的信息匹配：当文档被合理地分割后，检索算法可以更加精确的匹配用户查询和文档片段，不仅能减少噪声，还可以确保检索到的内容更加相关

 	3、增强生成质量：文本分割可以确保每个文档片段足够独立，且能够提供完整的上下文信息，从而提高生成的准确性和上下文连贯性

 	4、降低计算复杂度：减少模型计算量，提高响应速度，不仅优化了模型的性能，还减少了生成结果的延迟。

***2、LangChain中的 RecursivecharacterTextsplitter 相比 characterTextsplitter 的主要优势是什么?它是如何工作的?***

主要优势是分割方式更智能、更灵活。它会按优先顺序尝试用一组不同的分隔符（如段落、句子、空格）进行递归分割，尽可能保持语义单元（如句子）的完整性，而不是简单地按单一字符切分。



***3、文本分割器中的 chunk\_over1ap 参数有什么作用?为什么设置重叠通常是有益的?***

 	chunk\_overlap参数用于指定相邻文本块之间共享（重叠）的字符数。设置重叠是有益的，因为它有助于保持分割点附近的上下文连续性，避免重要信息在块的边界处被完全切断，从而利于后续的检索和理解。

***4、RecursivecharacterTextsplitter 默认的分隔符列表可能不适合中文文本，需要如何调整以获得更好的中文分割效果?***

 	需要在separators参数中加入中文常用的标点符号作为分隔符，例如句号。、逗号，、空格 等，并可以调整它们的优先级顺序，使得分割器优先在这些标点处切分。

***5、除了通用的按字符或递归字符分割，LangChain还提供了针对特定文档格式的分割器，例如这类分割器有什么优点?MarkdownHeaderTextSplitter***

 	优点在于能够利用文档自身的结构信息（如Markdown的标题 # ##）来进行分割。这样得到的文本块语义更连贯，并且可以在元数据（metadata）中保留原始的结构信息（如所属章节标题），有助于更精准地检索和组织信息。

6、三种方法：

 	1、字符分割：按照特定字符分割，如默认值换行符“\\n\\n”

 	2、递归字符分割：递归字符文本分割是字符分割的升级版，它以字符分割为基础，但在分割时引入了递归的机制。

 	3、特定文档的分割：特定文档分割是基于文档结构对文本进行分割的一种方式。不同的文档类型通常有各自的结构化信息，例如书籍中的章节和段落、网页中的HTML标签等。利用这些预定义的结构化信息，可以更加自然地分割本,保证片段的上下文连贯性。

#### 十、常见向量库数据库的介绍

1、什么是向量数据库？

 	向量数据库是一种专门用于存储和査询高维向量的数据库。随着机器学习和深度学习技术的发展，向量表示(如词向量、图像特征、用户行为向量等)变得越来越重要。这些向量通常是多维的，表示数据的特征和属性。

 	在这样的背景下，向量数据库提供了高效的存储、检索和相似度搜索功能。

向量数据库其实和其他数据库差不多，只不过他存的是向量。

 	一般使用向量数据库的流程是:创建数据库->创建集合->创建索引->插入数据->搜索。

***2、常见的向量数据库：***

 	1、Pinecone库：全托管的云端向量数据库服务

 	2、FAISS库：用于高效相似性搜索和稠密向量聚类的开源库

 	3、Chorma库：轻量级、嵌入式的开源向量数据库

 		4、Milvus库：云原生的、高性能的开源向量数据库

***3、为什么在构建RAG(检索增强生成)系统时，向量数据库是一个关键组件?***

因为RAG需要根据用户查询（转换成向量）快速地从大量文档（也转换成向量）中找到语义最相关的部分作为上下文提供给LLM。向量数据库专门优化了这种大规模向量的相似度搜索过程，能够高效、准确地完成检索任务。



***4、在向量数据库中进行相似度搜索，一般需要经历哪些基本步骤?***

数据向量化 -> 建库/索引 -> 入库 -> 查询向量化 -> 搜索。



***5、请简述开源库Faiss和云服务Pinecone在作为向量存储和检索方案时的主要区别。***

 	Faiss是一个本地库，提供高效的相似度搜索算法和索引结构，开发者需要自己集成和管理。Pinecone是一个托管的云服务（DBaaS），提供了完整的数据库管理功能和易用的API，用户无需关心底层运维。

***6、衡量两个向量相似度时常用的余弦相似度(Cosine Similarity)是基于什么原理?它的值域和含义是什么?***

余弦相似度基于计算两个向量之间夹角的余弦值。它的值域是 \\\[-1, 1]。值越接近1，表示两个向量的方向越相似；越接近-1表示方向越相反；接近0表示方向接近正交（不相关）。



#### 十一、RAG的向量化

***1、当使用FAISS索引执行 index.search(query\_embedding，k=5)时，返回结果  通常包含什么信息?***

 	index.search返回距离D和索引I，I包含的是找到的向量在原始添加顺序中的索引号

***2、在RAG的语境下，什么是文本向量化？***

 	文本向量化是将文本转换为高维数字向量的过程。这些向量能够捕捉文本的语义信息，使得语义相似的文本在向量空间中的表示也相近

***3、像FAISS这样的库在RAG的向量化和检索流程中扮演什么角色?***
FAISS扮演高效的向量索引和相似度搜索库的角色。在文本被向量化并存入后，FAISS能够快速地在海量向量中根据查询向量找到最相似的K个向量，从而实现快速检索。



#### 十二、langchain部署简单rag应用

***1、使用LangChain实现RAG应用，有哪两种主要的方法?它们的核心区别是什么?***

 	1. 使用内置的 RetrievalQA 链。、

 	2. 使用 LCEL（LangChain表达式语言）构建自定义管道。

 	核心区别在于：RetrievalQA 封装度高、使用简单，但灵活性低；LCEL 灵活性高、易于调试，但需要手动编排步骤，代码稍复杂

#### 十三、RAG的评估

1、评估RAG系统的性能主要涉及两个方面：检索的质量和生成的质量

2、检索模块的评估：

 	a、精确度和召回率：

 		精确度：在检索的文档中，正确相关的文档所占的比例

 		召回率：所有与用户查询相关的文档中，被成功检索到的比例

 		F1-score：精确度和召回率的调和平均数，常用于评估整体检索效果

 	b、排名质量（MRR）：

 		Mean Reciprocal Rank（MRR）是一种常用的评价指标，用于衡量信息检索系统，推荐系统或问答系统等任务中，系统返回的结果列表中正确答案的位置。MRR特别适合用于需要评估多个查询或任务的综合性能的情况

 		计算步骤：（1）确定每个查询的第一个正确答案的位置

 				（2）计算每个查询的倒数

 				（3）求平均值

 	c、平均准确率（AP）：用于评估单个查询的检索结果质量，它计算的是在每个相关文档出现的的位置之前的平均准确率

 	d、平均平均准确率（MAP）：用于评估多个查询的检索结果质量

3、生成模块的评估：

生成模块的评估关注生成的文本是否准确、连贯且符合用户需求。

自动化评估指标。

 	BLEU(Bilingual Evaluation Understudy):主要用于机器翻译，但也常用于生成任务。它通过比较生成文本与参考答案之间的词汇 n-gram 的匹配程度来评估生成文本的质量。BLEU 的分值在0 到1之间，分数越高表示生成文本与参考文本越接近。

 	ROUGE(Recall-Oriented Understudy for Gisting Evalùation):常用于摘要任务，主要评估生成文本与参考文本之间的重叠情况，主要关注召回率。常用的指标有 ROUGE-N(n-gram 匹配)、ROUGE-L(最长公共子序列匹配)等。

 	更直观的评估指标：

 	（1）响应参考答案：

 		衡量RAG答案相对于真实答案的相似/正确程度，是否回答准确

 	（2）响应与输入：

 		衡量生成的响应与初始化输入的相关程度，是否回答准确

 	（3）响应与检索到的文档：

 		衡量生成的响应与检索到的文档相关性，是否检索正确

 	（4）检索到的文档与输入：

 		衡量我检索到的文档与查询的相关性，是否检索正确



####  十四、RAG的优化

1、优化分为两个模块：检索模块和生成模块

2、可以通过哪些评估指标来判断应该侧重优化哪个模块？

 	如果评估指标中上下文相关性/召回率（如Context Precision/Recall）低，应优化检索模块，

 	如果答案忠实度/相关性（Faithfulness/Answer Relavancy）低，应优化生成模块

3、什么是混合检索？它结合了那两种检索方式的优点？

 	混合检索是同时使用的稠密检索（基于语义向量）和稀疏检索（基于关键词，如BM25）并将结果融合的方法，

 	它结合了稠密检索的语义理解能力和稀疏检索的关键词精确匹配能力，以期望获得更全面，更鲁邦的效果

4、为了提高检索效果，可以对用户的原始查询进行优化，文档介绍了哪些查询优化技术？

 	查询优化技术包括：

 		1、问题重写（改写或补充信息）

 		2、后退提示（step-back prompting）生成更泛华的问题，

 		3、多查询（Multi-Query）从不同角度生成多个问题

4、在RAG流程中，Rerank（重新排序）步骤的作用是什么？

 	Rerank的作用是对初步检索（如向量搜索）返回的候选文档列表进二次，更精确的相关度排序，将最相关的文档排在前面，以提升最终送入生成模型的上下文质量。它通常在初步检索之后，生成之前执行。

#### 十五、总结

***一、在实现RAG的过程中，如何选择词嵌入模型和大语言模型？为什么选择某种模型，对比其他的模型它的优势在哪里？***

&nbsp;	核心：词嵌入模型负责“找的准”，大语言模型负责“答的好”

&nbsp;	词嵌入选择：追求性能和极致效果选BGE-large，对比一个梯队的openai embedding-3-large，BGE是开源模型，它的成本很低，并且在处理中文的表现上非常出色，数据隐私可控，作为词嵌入模型是首选。

均衡选择的话可以选择BGE-base，是中文 RAG 的默认推荐起点，它在保持接近 large 模型性能的同时，速度快得多。

&nbsp;	选开源模型在做垂直领域工作时更具有优势，在一些行业的专业术语，通用模型可能无法理解，这时候对模型进行微调训练，可以显著提升领域内检索的准确率，这是闭源模型无法做到的。

大语言模型：选择Qwen2.5-7B instruct

&nbsp;	原因：1、在开源模型中，Qwen2.5-7B instruct性能强悍，不管是代码，数学，综合表现超越同尺寸模型，甚至其性能逼近一些20B+模型

&nbsp;		2、上下文窗口巨大，它支持128K上下文，远超同级别模型    

&nbsp;		3、中英双语理解能力出色，对中文的理解和生成能力，英语也同样出色，相比Llama模型，它在英语方面更强，中文能力一般

***二、如何基于RAG的评估去进行优化？***

检索质量指标的优化

（一）基于精确率低的情况：可能是因为噪声文档过多，干扰模型检索生成

优化方案：1、重排序，使用Rerank模型对初次检索结果进行更精确排序

&nbsp;		2、查询优化：mutil query（多查询），实施查询重写，让查询更精确

&nbsp;		HyDE：根据用户指令查询想象并生成一个假设的答案，与真实文档进行检索，选出真是文档与原始查询一起交给LLM

&nbsp;		3、增加元数据索引：在为文本块创建向量索引时，为其附加结构化信息（元数据），如来源，作者，创建日期，标题等等。

（二）基于召回率低，就是漏检索了相关文档

优化方案：1、扩展查询，使用Mutil Query技术，去生成3-5个表述不同，角度不同的新查询

&nbsp;		2、分块策略优化，尝试语义分块，比如递归字符分割，根据语义边界分块；减小分块大小；使用重叠分块，每块前后增加一定冗余，防止语义断裂

&nbsp;		3、增强检索能力，采用混合检索（BM25）+向量检索

扩展：BM25原理

（三）MRR低，相关文档排名靠后

优化方案：1、重排序模型，

&nbsp;		2、改进嵌入模型

&nbsp;		3、调整相似度算法

生成质量指标的优化：

（一）答案相关性低---答案与问题不匹配

优化方案：1、Prompt工程

&nbsp;		2、temperature，top\_p

（二）事实准确性低---存在幻觉

优化方案：1、强化约束:在prompt中明确要求"只基于提供的上下文

&nbsp;		2、引用机制:要求模型标注答案来源的原文位置

&nbsp;		3、后处理验证:对关键事实进行二次检索验证

（三）忠实度低---偏离检索内容

优化方案：1、更换LLM:使用指令遵循能力更强的模型(如GPT-4>GPT-3.5)

&nbsp;		2、上下文压缩:只将最相关的文档片段送入LLM，减少噪声干扰

&nbsp;		3、分步生成:先让LLM提取关键证据，再基于证据生成答案



